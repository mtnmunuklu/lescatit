<!-- Code generated by gomarkdoc. DO NOT EDIT -->

# crawler

```go
import "Lescatit/crawler"
```

## Index



# models

```go
import "Lescatit/crawler/models"
```

## Index

- [type Crawler](<#type-crawler>)
  - [func (c *Crawler) FromProtoBuffer(category *pb.Crawler)](<#func-crawler-fromprotobuffer>)
  - [func (c *Crawler) ToProtoBuffer() *pb.Crawler](<#func-crawler-toprotobuffer>)


## type [Crawler](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/models/crawlers.go#L11-L19>)

Crawler provides the category instance for crawlers job\.

```go
type Crawler struct {
    Id       bson.ObjectId `bson:"_id"`
    Url      string        `bson:"url"`
    Category string        `bson:"category"`
    Created  time.Time     `bson:"created"`
    Updated  time.Time     `bson:"updated"`
    Revision string        `bson:"revision"`
    Data     string        `bson:"data"`
}
```

### func \(\*Crawler\) [FromProtoBuffer](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/models/crawlers.go#L35>)

```go
func (c *Crawler) FromProtoBuffer(category *pb.Crawler)
```

FromProtoBuffer gets data from protocol buffer and converts to the crawler structure\.

### func \(\*Crawler\) [ToProtoBuffer](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/models/crawlers.go#L22>)

```go
func (c *Crawler) ToProtoBuffer() *pb.Crawler
```

ToProtoBuffer converts the crawler structure into a protocol buffer crawler structure\.

# repository

```go
import "Lescatit/crawler/repository"
```

## Index

- [Constants](<#constants>)
- [type CRepository](<#type-crepository>)
  - [func (r *CRepository) DeleteAll() error](<#func-crepository-deleteall>)
  - [func (r *CRepository) GetById(id string) (url *models.Crawler, err error)](<#func-crepository-getbyid>)
  - [func (r *CRepository) GetDataByURL(url string) (data *models.Crawler, err error)](<#func-crepository-getdatabyurl>)
  - [func (r *CRepository) Save(url *models.Crawler) error](<#func-crepository-save>)
- [type CrawlersRepository](<#type-crawlersrepository>)
  - [func NewCrawlersRepository(conn db.Connection) CrawlersRepository](<#func-newcrawlersrepository>)


## Constants

```go
const CrawlersCollection = "categories"
```

## type [CRepository](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/repository/crawlers.go#L21-L23>)

CRepository provides a mongo collection for database job\.

```go
type CRepository struct {
    // contains filtered or unexported fields
}
```

### func \(\*CRepository\) [DeleteAll](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/repository/crawlers.go#L48>)

```go
func (r *CRepository) DeleteAll() error
```

DeleteAll drops crawlers collection\.

### func \(\*CRepository\) [GetById](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/repository/crawlers.go#L36>)

```go
func (r *CRepository) GetById(id string) (url *models.Crawler, err error)
```

GetById returns the url based on id\.

### func \(\*CRepository\) [GetDataByURL](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/repository/crawlers.go#L42>)

```go
func (r *CRepository) GetDataByURL(url string) (data *models.Crawler, err error)
```

GetDataByURL returns the data based on url\.

### func \(\*CRepository\) [Save](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/repository/crawlers.go#L31>)

```go
func (r *CRepository) Save(url *models.Crawler) error
```

Save adds url to database\.

## type [CrawlersRepository](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/repository/crawlers.go#L14-L18>)

CrawlersRepository is the interface of the crawler backend\.

```go
type CrawlersRepository interface {
    Save(url *models.Crawler) error
    GetById(id string) (url *models.Crawler, err error)
    GetDataByURL(url string) (data *models.Crawler, err error)
}
```

### func [NewCrawlersRepository](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/repository/crawlers.go#L26>)

```go
func NewCrawlersRepository(conn db.Connection) CrawlersRepository
```

NewCrawlersRepository creates a new CrawlersRepository instance\.

# scraper

```go
import "Lescatit/crawler/scraper"
```

## Index

- [type CScraper](<#type-cscraper>)
  - [func (cs *CScraper) FromProtoBuffer(crawler *pb.CrawlRequest)](<#func-cscraper-fromprotobuffer>)
  - [func (cs *CScraper) GetData(url string) (string, error)](<#func-cscraper-getdata>)
  - [func (cs *CScraper) GetLinks(url string) ([]string, error)](<#func-cscraper-getlinks>)
  - [func (cs *CScraper) Init()](<#func-cscraper-init>)
- [type CollyScraper](<#type-collyscraper>)
  - [func NewCollyScraper() CollyScraper](<#func-newcollyscraper>)


## type [CScraper](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/scraper/scraper.go#L19-L29>)

```go
type CScraper struct {
    // contains filtered or unexported fields
}
```

### func \(\*CScraper\) [FromProtoBuffer](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/scraper/scraper.go#L115>)

```go
func (cs *CScraper) FromProtoBuffer(crawler *pb.CrawlRequest)
```

FromProtoBuffer gets data from protocol buffer and converts to the cscraper structure\.

### func \(\*CScraper\) [GetData](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/scraper/scraper.go#L88>)

```go
func (cs *CScraper) GetData(url string) (string, error)
```

GetData provides to get the content in the url address\.

### func \(\*CScraper\) [GetLinks](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/scraper/scraper.go#L49>)

```go
func (cs *CScraper) GetLinks(url string) ([]string, error)
```

GetLinks provides to get the links in the url address\.

### func \(\*CScraper\) [Init](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/scraper/scraper.go#L40>)

```go
func (cs *CScraper) Init()
```

Init initializes the CScraper's private variables and sets default configuration for the CSraper

## type [CollyScraper](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/scraper/scraper.go#L13-L17>)

```go
type CollyScraper interface {
    GetLinks(url string) ([]string, error)
    GetData(url string) (string, error)
    FromProtoBuffer(crawler *pb.CrawlRequest)
}
```

### func [NewCollyScraper](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/scraper/scraper.go#L32>)

```go
func NewCollyScraper() CollyScraper
```

NewCollyScraper creates a new CollyScraper instance with default configuration\.

# service

```go
import "Lescatit/crawler/service"
```

## Index

- [func NewCrawlService(crawlersRepository repository.CrawlersRepository, collyScraper scraper.CollyScraper) pb.CrawlServiceServer](<#func-newcrawlservice>)
- [type CrawlService](<#type-crawlservice>)
  - [func (s *CrawlService) CrawlURL(ctx context.Context, req *pb.CrawlURLRequest) (*pb.CrawlURLResponse, error)](<#func-crawlservice-crawlurl>)
  - [func (s *CrawlService) CrawlURLs(req *pb.CrawlURLsRequest, stream pb.CrawlService_CrawlURLsServer) error](<#func-crawlservice-crawlurls>)
  - [func (s *CrawlService) GetURLData(ctx context.Context, req *pb.GetURLDataRequest) (*pb.GetURLDataResponse, error)](<#func-crawlservice-geturldata>)
  - [func (s *CrawlService) GetURLsData(req *pb.GetURLsDataRequest, stream pb.CrawlService_GetURLsDataServer) error](<#func-crawlservice-geturlsdata>)


## func [NewCrawlService](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/service/service.go#L20>)

```go
func NewCrawlService(crawlersRepository repository.CrawlersRepository, collyScraper scraper.CollyScraper) pb.CrawlServiceServer
```

NewCrawlService creates a new CrawlService instance\.

## type [CrawlService](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/service/service.go#L14-L17>)

CrawlService provides crawlersRepository and collyScraper for crawler service\.

```go
type CrawlService struct {
    // contains filtered or unexported fields
}
```

### func \(\*CrawlService\) [CrawlURL](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/service/service.go#L101>)

```go
func (s *CrawlService) CrawlURL(ctx context.Context, req *pb.CrawlURLRequest) (*pb.CrawlURLResponse, error)
```

CrawlURL performs crawl the url

### func \(\*CrawlService\) [CrawlURLs](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/service/service.go#L115>)

```go
func (s *CrawlService) CrawlURLs(req *pb.CrawlURLsRequest, stream pb.CrawlService_CrawlURLsServer) error
```

CrawlUrls performs crawl the urls

### func \(\*CrawlService\) [GetURLData](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/service/service.go#L25>)

```go
func (s *CrawlService) GetURLData(ctx context.Context, req *pb.GetURLDataRequest) (*pb.GetURLDataResponse, error)
```

GetURLData provides to get the content in the url address\.

### func \(\*CrawlService\) [GetURLsData](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/service/service.go#L59>)

```go
func (s *CrawlService) GetURLsData(req *pb.GetURLsDataRequest, stream pb.CrawlService_GetURLsDataServer) error
```

GetURLsData provides to get the content in the url addresses\.

# util

```go
import "Lescatit/crawler/util"
```

## Index

- [Variables](<#variables>)
- [func ValidateURL(reqURL string) error](<#func-validateurl>)
- [func ValidateURLs(urls []string) error](<#func-validateurls>)


## Variables

Contains error codes for crawler service\.

```go
var (
    ErrInvalidURL  = errors.New("invalid url")
    ErrEmptyURLs   = errors.New("urls can't be empty")
    ErrURLNotExist = errors.New("url does not exist")
)
```

## func [ValidateURL](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/util/util.go#L16>)

```go
func ValidateURL(reqURL string) error
```

ValidateURLs validates if it's a real url\.

## func [ValidateURLs](<https://github.com/mtnmunuklu/Lescatit/blob/main/crawler/util/util.go#L25>)

```go
func ValidateURLs(urls []string) error
```

ValidateURLs validates the url count\.



Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)
